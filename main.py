import streamlit as st
from utils import qa_agent
from langchain.memory import ConversationBufferMemory

st.header("ğŸ“ƒ æ™ºèƒ½PDFé—®ç­”å·¥å…·")

with st.sidebar:
    openai_api_key = st.text_input("è¯·è¾“å…¥OpenAI APIå¯†é’¥ï¼š", type="password")
    st.markdown("[è·å–OpenAI APIå¯†é’¥](https://api.aigc369.com/register?aff=87kh)")

#ä½¿ç”¨ä¼šè¯çŠ¶æ€é¿å…æ¯æ¬¡æäº¤åˆå§‹åŒ–memory
if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(return_messages=True,
                                                          memory_key="chat_history",
                                                          output_key="answer")

uploaded_file = st.file_uploader("ä¸Šä¼ ä½ çš„PDFæ–‡ä»¶ï¼š", type="pdf")
question = st.text_input("å¯¹PDFçš„å†…å®¹è¿›è¡Œæé—®", disabled=not uploaded_file)    #disabledæ§åˆ¶è¾“å…¥æ¡†æ˜¯å¦å…è®¸è¾“å…¥

if uploaded_file and question:
    if not openai_api_key:
        st.info("è¯·è¾“å…¥ä½ çš„OpenAI APIå¯†é’¥")
        st.stop()

    with st.spinner("AIæ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨ç­‰..."):
        result = qa_agent(uploaded_file, question, st.session_state["memory"], openai_api_key)

    st.write("### ç­”æ¡ˆ")
    st.write(result["answer"])
    #æŠŠå†å²å¯¹è¯æ”¾è¿›è®°å¿†ä¸­
    st.session_state["chat_history"] = result["chat_history"]

if "chat_history" in st.session_state:
    with st.expander("å†å²æ¶ˆæ¯"):
        for i in range(0, len(st.session_state["chat_history"]), 2):
            human_message = st.session_state["chat_history"][i]
            ai_message = st.session_state["chat_history"][i+1]
            st.write(human_message.content)
            st.write(ai_message.content)
            if i < len(st.session_state["chat_history"])-2:
                st.divider()